tmps_env __init__
(60.0, 110.0)
(200.0, 140.0)
(60.0, 240.0)
(210.0, 220.0)
add_agent
add_agent
add_agent
add_agent
render_data shape (600, 300, 3)
render_data shape2 (600, 300, 3)
{'num_agents': 4, 'obs_box_size': 50, 'init_pos': ((60.0, 110.0), (200.0, 140.0), (60.0, 240.0), (210.0, 220.0)), 'dynamic_delta_t': 0.01, 'map_width': 300, 'map_height': 300, 'geo_grid_data': <envs.bases.geo_base.GeoGridData object at 0x7fd1cc6e1700>, 'agents': [<envs.bases.agent_base.AgentInterface object at 0x7fd1cc6cb8e0>, <envs.bases.agent_base.AgentInterface object at 0x7fd1cc6f2070>, <envs.bases.agent_base.AgentInterface object at 0x7fd1cc6e1ee0>, <envs.bases.agent_base.AgentInterface object at 0x7fd1cc6d37f0>], 'enemies': [<envs.bases.scenario_base.EnemyInterface object at 0x7fd1cc6cb9a0>, <envs.bases.scenario_base.EnemyInterface object at 0x7fd1cc6cbb20>, <envs.bases.scenario_base.EnemyInterface object at 0x7fd1cc6911c0>, <envs.bases.scenario_base.EnemyInterface object at 0x7fd1cc73ccd0>]}
=============================
=1 Env simple_adversary is right ...
=============================
=2 The 4 agents are inited ...
=============================
=3 starting iterations ...
=============================
steps:0 episode:0
/home/jonghae/ai_crew_project/MADDPG/model.py:116: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/torch/csrc/utils/tensor_new.cpp:261.)
  self.amplitude = torch.tensor([(self.action_space[0].high - self.action_space[0].low) / 2.0, (self.action_space[1].high - self.action_space[1].low) / 2.0], device=args.device ,dtype=torch.float32)
IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 300) to (608, 304) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).
evaluation is finished at 0 th episode
episode reward:-108.199 agents mean reward:[-49.65, -22.36, -21.19, -15.0]
steps:1000 episode:1
steps:2000 episode:2
steps:3000 episode:3
steps:4000 episode:4
steps:5000 episode:5
steps:6000 episode:6
steps:7000 episode:7
steps:8000 episode:8
steps:9000 episode:9
steps:10000 episode:10
=start training ...
steps:11000 episode:11
steps:12000 episode:12
steps:13000 episode:13
steps:14000 episode:14
steps:15000 episode:15
steps:16000 episode:16
steps:17000 episode:17
steps:18000 episode:18
steps:19000 episode:19
steps:20000 episode:20
Traceback (most recent call last):
  File "/home/jonghae/ai_crew_project/MADDPG/main_openai.py", line 344, in <module>
    train(arglist, video)
  File "/home/jonghae/ai_crew_project/MADDPG/main_openai.py", line 233, in train
    action_n = [agent(torch.from_numpy(obs).to(arglist.device, torch.float)).detach().cpu().numpy() \
  File "/home/jonghae/ai_crew_project/MADDPG/main_openai.py", line 233, in <listcomp>
    action_n = [agent(torch.from_numpy(obs).to(arglist.device, torch.float)).detach().cpu().numpy() \
  File "/home/jonghae/anaconda3/envs/aicrew/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jonghae/anaconda3/envs/aicrew/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1521, in _call_impl
    forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
KeyboardInterrupt